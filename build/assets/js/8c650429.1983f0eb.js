"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[1020],{6858(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-3-ai-robot-brain/index","title":"Module 3: The AI-Robot Brain","description":"This module focuses on Embodied Intelligence. We transition from basic mechanics to high-level perception and path planning using NVIDIA\'s ecosystem.","source":"@site/docs/module-3-ai-robot-brain/index.md","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/","permalink":"/robotics-ai-book/docs/module-3-ai-robot-brain/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-robot-brain/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Synthetic Sensors in Simulation","permalink":"/robotics-ai-book/docs/module-2-digital-twin/sensors"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/robotics-ai-book/docs/module-4-vla-capstone/"}}');var a=i(4848),s=i(8453);const t={},r="Module 3: The AI-Robot Brain",l={},d=[{value:"NVIDIA Isaac Sim &amp; USD",id:"nvidia-isaac-sim--usd",level:2},{value:"Isaac ROS: VSLAM &amp; Nav2",id:"isaac-ros-vslam--nav2",level:2},{value:"Weekly Breakdown",id:"weekly-breakdown",level:2},{value:"Week 6: Computer Vision",id:"week-6-computer-vision",level:3},{value:"Week 7: VSLAM &amp; Navigation",id:"week-7-vslam--navigation",level:3},{value:"Week 8: Photo-realistic Rendering",id:"week-8-photo-realistic-rendering",level:3}];function c(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"module-3-the-ai-robot-brain",children:"Module 3: The AI-Robot Brain"})}),"\n",(0,a.jsxs)(n.p,{children:["This module focuses on ",(0,a.jsx)(n.strong,{children:"Embodied Intelligence"}),". We transition from basic mechanics to high-level perception and path planning using NVIDIA's ecosystem."]}),"\n",(0,a.jsx)(n.h2,{id:"nvidia-isaac-sim--usd",children:"NVIDIA Isaac Sim & USD"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"NVIDIA Isaac Sim"})," is built on the ",(0,a.jsx)(n.strong,{children:"Universal Scene Description (USD)"})," format."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"USD Advantage:"})," Allows for non-destructive editing and collaborative 3D workflows between engineers and artists."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Workflow:"})," Export your robot from URDF to USD for high-fidelity photorealistic simulation."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-vslam--nav2",children:"Isaac ROS: VSLAM & Nav2"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS"})," provides hardware-accelerated packages for common robotic tasks:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VSLAM (Visual Simultaneous Localization and Mapping):"})," Uses camera feeds to determine the robot's position in an unknown environment."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Nav2 (Navigation 2):"})," A modular navigation stack that handles obstacle avoidance and global path planning."]}),"\n"]}),"\n",(0,a.jsx)(n.admonition,{title:"Sim-to-Real Transfer",type:"tip",children:(0,a.jsxs)(n.p,{children:["To ensure algorithms learned in simulation work on physical hardware (",(0,a.jsx)(n.a,{href:"/robotics-ai-book/docs/hardware-lab/",children:"Hardware Lab Guide"}),"), use ",(0,a.jsx)(n.strong,{children:"Domain Randomization"}),". This involves varying light, textures, and physics parameters during training so the AI doesn't overfit to the virtual world."]})}),"\n",(0,a.jsx)(n.h2,{id:"weekly-breakdown",children:"Weekly Breakdown"}),"\n",(0,a.jsx)(n.h3,{id:"week-6-computer-vision",children:"Week 6: Computer Vision"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"GPU-accelerated OpenCV integration."}),"\n",(0,a.jsx)(n.li,{children:"Training object detection models for robotic interaction."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"week-7-vslam--navigation",children:"Week 7: VSLAM & Navigation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implementing real-time mapping."}),"\n",(0,a.jsx)(n.li,{children:"Tuning Nav2 parameters for humanoid balance."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"week-8-photo-realistic-rendering",children:"Week 8: Photo-realistic Rendering"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Advanced light modeling in Isaac Sim."}),"\n",(0,a.jsx)(n.li,{children:"Testing vision models against complex synthetic backgrounds."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>r});var o=i(6540);const a={},s=o.createContext(a);function t(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);