"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[8757],{8172(e,n,o){o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla-capstone/index","title":"Module 4: Vision-Language-Action (VLA)","description":"The final module explores the convergence of Large Language Models (LLMs) and physical robotics, creating robots that can understand and execute human instructions.","source":"@site/docs/module-4-vla-capstone/index.md","sourceDirName":"module-4-vla-capstone","slug":"/module-4-vla-capstone/","permalink":"/robotics-ai-book/docs/module-4-vla-capstone/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla-capstone/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain","permalink":"/robotics-ai-book/docs/module-3-ai-robot-brain/"}}');var t=o(4848),a=o(8453);const s={},r="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"The VLA Pipeline",id:"the-vla-pipeline",level:2},{value:"Capstone Project Requirements",id:"capstone-project-requirements",level:2},{value:"Weekly Breakdown",id:"weekly-breakdown",level:2},{value:"Week 9: Cognitive Planning",id:"week-9-cognitive-planning",level:3},{value:"Week 10: Motion Control",id:"week-10-motion-control",level:3},{value:"Week 11-12: VLA Integration &amp; Capstone",id:"week-11-12-vla-integration--capstone",level:3},{value:"Week 13: Final Lab Demo",id:"week-13-final-lab-demo",level:3}];function d(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.p,{children:"The final module explores the convergence of Large Language Models (LLMs) and physical robotics, creating robots that can understand and execute human instructions."}),"\n",(0,t.jsx)(n.h2,{id:"the-vla-pipeline",children:"The VLA Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"We move beyond hard-coded scripts to conversational robotics using a three-stage pipeline:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception (Whisper):"})," Converts spoken human commands into text."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognition (LLM):"}),' Reasons about the command and decomposes it into sub-tasks (e.g., "Go to the kitchen" -> 1. Locate door, 2. Navigate, 3. Detect kitchen).']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action (ROS 2):"})," Sends velocity and joint commands to the physical or simulated actuators."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project-requirements",children:"Capstone Project Requirements"}),"\n",(0,t.jsx)(n.p,{children:'The final project requires building a "Conversational Humanoid Agent."'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Features Required:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Autonomous navigation through a dynamic environment."}),"\n",(0,t.jsx)(n.li,{children:'Voice-activated task execution (e.g., "Pick up the blue cube").'}),"\n",(0,t.jsx)(n.li,{children:"Real-time obstacle avoidance."}),"\n",(0,t.jsx)(n.li,{children:"Status reporting back via speech synthesis."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Refer to ",(0,t.jsx)(n.a,{href:"/robotics-ai-book/docs/hardware-lab/",children:"Hardware Lab"})," for Tier Options (B or C) suitable for this capstone."]}),"\n",(0,t.jsx)(n.h2,{id:"weekly-breakdown",children:"Weekly Breakdown"}),"\n",(0,t.jsx)(n.h3,{id:"week-9-cognitive-planning",children:"Week 9: Cognitive Planning"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Interfacing GPT-4o or Claude 3.5 with the ROS 2 environment."}),"\n",(0,t.jsx)(n.li,{children:"Writing task decomposition prompts."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"week-10-motion-control",children:"Week 10: Motion Control"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Implementing ",(0,t.jsx)(n.strong,{children:"Reinforcement Learning"})," for bipedal gait."]}),"\n",(0,t.jsx)(n.li,{children:"Balance recovery algorithms in Isaac Sim."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"week-11-12-vla-integration--capstone",children:"Week 11-12: VLA Integration & Capstone"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrating Whisper for speech-to-text."}),"\n",(0,t.jsx)(n.li,{children:"Finalizing the full perception-cognition-action loop."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"week-13-final-lab-demo",children:"Week 13: Final Lab Demo"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Live demonstration of the autonomous conversational agent."}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,o){o.d(n,{R:()=>s,x:()=>r});var i=o(6540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);