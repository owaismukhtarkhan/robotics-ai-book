# ماڈیول 4: ویژن-لینگویج-ایکشن (VLA)

آخری ماڈیول میں ہم لارج لینگویج ماڈلز (LLMs) اور فزیکل روبوٹکس کے ملاپ کا مطالعہ کریں گے، جس سے ایسے روبوٹ تیار ہوں گے جو انسانی ہدایات کو سمجھ اور ان پر عمل کر سکیں۔

## VLA پائپ لائن

ہم سادہ اسکرپٹس سے آگے بڑھ کر "کنورسیشنل روبوٹکس" (Conversational Robotics) کی طرف بڑھتے ہیں:

1. **ادراک (Whisper):** انسانی آواز کے احکامات کو ٹیکسٹ میں تبدیل کرتا ہے۔
2. **فہم (LLM):** حکم پر غور کرتا ہے اور اسے چھوٹے کاموں میں تقسیم کرتا ہے (مثال: "باورچی خانے میں جاؤ" -> 1. دروازہ تلاش کرو، 2. راستہ طے کرو، 3. کچن کی شناخت کرو)۔
3. **عمل (ROS 2):** فزیکل یا سمیلیٹڈ روبوٹ کو حرکت کے احکامات بھیجتا ہے۔

## کیپ اسٹون پروجیکٹ کے تقاضے

فائنل پروجیکٹ میں ایک "خود مختار کنورسیشنل ایجنٹ" بنانا شامل ہے۔

**ضروری خصوصیات:**
- متحرک ماحول میں خود مختار نیویگیشن۔
- آواز کے ذریعے کاموں کی انجام دہی (مثال: "نیلا کیوب اٹھاؤ")۔
- رکاوٹوں سے بچنے کی صلاحیت۔
- آواز کے ذریعے رپورٹنگ۔

ہارڈ ویئر کے انتخاب کے لیے [ہارڈ ویئر لیب](/docs/hardware-lab/index.md) کے ٹیئر بی یا سی دیکھیں۔

## ہفتہ وار جائزہ

### ہفتہ 9: علمی منصوبہ بندی (Cognitive Planning)
- GPT-4o یا Claude 3.5 کو ROS 2 کے ساتھ جوڑنا۔
- کاموں کی تقسیم کے لیے پرامپٹ لکھنا۔

### ہفتہ 10: موشن کنٹرول
- چلنے کے لیے **Reinforcement Learning** کا نفاذ۔
- Isaac Sim میں توازن برقرار رکھنے کے الگورتھم۔

### ہفتہ 11-12: VLA انضمام
- آواز سے ٹیکسٹ کے لیے Whisper کا استعمال۔
- ادراک، فہم اور عمل کے پورے لوپ کو مکمل کرنا۔

### ہفتہ 13: فائنل لیب ڈیمو
- خود مختار ایجنٹ کا لائیو مظاہرہ۔
